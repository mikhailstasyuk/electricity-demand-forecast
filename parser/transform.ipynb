{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import optuna\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads data from a pickle file, removes unused columns.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Pandas DataFrame object.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO Verify data format and pre-defined schema.\n",
    "\n",
    "    cols_to_drop = ['subba-name', 'parent', 'parent-name', 'value-units']\n",
    "    dtypes = {'period': 'datetime64',\n",
    "              'subba': 'category',\n",
    "              'timezone': 'category',\n",
    "              'value': 'uint64'}\n",
    "\n",
    "    # Load the data, remove extra columns, and adjust data types.\n",
    "    df = pd.read_pickle(filename).drop(columns=cols_to_drop)\n",
    "    df = df.sort_values(by=['period'], ascending=True)\n",
    "    df = df.drop_duplicates().dropna()\n",
    "    df = df.astype(dtypes)\n",
    "    \n",
    "    # We focus solely on New York City data for our task.\n",
    "    df = df[df['subba'] == 'ZONJ']\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# df = read_data(filename='data/2018-01-01-2022-01-01.bin')\n",
    "# print('df shape:', df.shape)\n",
    "# print(df.head(1))\n",
    "# print(df.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_iqr(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter DataFrame using the Interquartile Range (IQR) method.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the \"value\" column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    # Compute the IQR for the \"value\" column\n",
    "    Q1 = df['value'].quantile(0.25)\n",
    "    Q3 = df['value'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define bounds for the acceptable range\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Remove rows where the \"value\" column is outside the bounds.\n",
    "    filt = (df['value'] >= lower_bound) & (df['value'] <= upper_bound)\n",
    "    df_filtered = df[filt]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def plot_data(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Plot the data from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'period' and 'value' columns.\n",
    "    \"\"\"\n",
    "    # Sort the DataFrame by \"period\" and plot the data\n",
    "    x = 'period'\n",
    "    y = 'value'\n",
    "    \n",
    "    df.sort_values(by='period').plot(x=x, y=y, figsize=(10, 5))\n",
    "\n",
    "    # Set the axis labels and title\n",
    "    plt.xlabel('Period')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Value over Time')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# plot_data(df_no_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "        df: pd.DataFrame,\n",
    "        window_size=7,\n",
    "        inference=False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract date-related and rolling statistics features from DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'period' and 'value' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added date features and rolling stats.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Separate date features\n",
    "    df['year'] = df['period'].dt.year.astype('uint16')\n",
    "    df['month'] = df['period'].dt.month.astype('uint16')\n",
    "    df['day'] = df['period'].dt.day.astype('uint16')\n",
    "\n",
    "    # Extract the day of the week\n",
    "    df['day_of_week'] = df['period'].dt.day_name().astype('category')\n",
    "\n",
    "    # Extract quarterly and weekly information to capture seasonality\n",
    "    df['quarter'] = df['period'].dt.quarter.astype('uint8')\n",
    "    df['week_of_year'] = df['period'].dt.isocalendar().week.astype('uint16')\n",
    "\n",
    "    # Mark the weekends\n",
    "    df['is_weekend'] = (df['period'].dt.weekday >= 5).astype('uint8')\n",
    "    \n",
    "    if not inference:\n",
    "        # Create rolling mean feature\n",
    "        df['rolling_mean'] = df['value'].rolling(window=window_size).mean()\n",
    "    \n",
    "        # Create rolling standard deviation feature\n",
    "        df['rolling_std'] = df['value'].rolling(window=window_size).std()\n",
    "    \n",
    "    # Remove NaNs introduced by the rolling mean and std\n",
    "    df = df.dropna().copy().reset_index(drop=True)\n",
    "\n",
    "    # Create a random feature as a threshold for later feature filtering\n",
    "    random_feature = [random.random() for _ in range(df.shape[0])]\n",
    "    df['random_feature'] = random_feature\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_supervised(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform DataFrame into a supervised learning format.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'lag' column and 'period' dropped.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create 'lag' column by shifting 'value' column\n",
    "    df.loc[:, 'lag'] = df['value'].shift()\n",
    "\n",
    "    # Remove rows with missing values (NaN) due to the shift\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Convert 'lag' column to unsigned 64-bit integer type\n",
    "    df.lag = df.lag.astype('uint64')\n",
    "\n",
    "    # Drop the 'period' column from the DataFrame\n",
    "    df = df.drop(columns=['period'])\n",
    "    print('LAST:', df.iloc[0])\n",
    "    return df\n",
    "\n",
    "def encode_categorical(df: pd.DataFrame, ohe=None, fit=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encode categorical columns as dummy variables.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with categorical columns encoded as dummy variables.\n",
    "    \"\"\" \n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    if ohe == None:\n",
    "        ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    # List of columns to encode as dummy variables\n",
    "    feat_to_enc = df.select_dtypes('category')\n",
    "\n",
    "    # Get dummy variables for the specified columns\n",
    "    if fit == True:\n",
    "        dummies = pd.DataFrame(ohe.fit_transform(feat_to_enc))\n",
    "    else:\n",
    "        dummies = pd.DataFrame(ohe.transform(feat_to_enc))\n",
    "    dummies.columns = ohe.get_feature_names_out()\n",
    "    \n",
    "    # Concatenate the dummy variables with the original DataFrame\n",
    "    df = pd.concat((df.drop(columns=feat_to_enc.columns), dummies), axis=1)\n",
    "    return df, ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: XGBRegressor, \n",
    "    X: pd.DataFrame, \n",
    "    y: pd.Series,\n",
    "    n_splits: int\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train a model using TimeSeriesSplit cross-validation.\n",
    "\n",
    "    Args:\n",
    "        model (XGBRegressor): XGBoost regressor using scikit-learn API.\n",
    "        X (pd.DataFrame): Features DataFrame.\n",
    "        y (pd.Series): Target values Series.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains average Mean Absolute Error on the training \n",
    "              set (float) and test set (float) respectively.\n",
    "    \"\"\"\n",
    "    # Lists to store mean absolute errors for training and test sets\n",
    "    mae_train_hist = []\n",
    "    mae_test_hist = []\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    # Split the data using TimeSeriesSplit for cross-validation\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        # Split the data into training and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "        \n",
    "        # Predict on training and test sets\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Calculate mean absolute error for training and test sets\n",
    "        mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "        mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "        \n",
    "        # Append the errors to their respective histories\n",
    "        mae_train_hist.append(mae_train)\n",
    "        mae_test_hist.append(mae_test)\n",
    "\n",
    "    # Calculate average mean absolute error for training and test sets\n",
    "    mae_train_avg = sum(mae_train_hist) / len(mae_train_hist)\n",
    "    mae_test_avg = sum(mae_test_hist) / len(mae_test_hist)\n",
    "    \n",
    "    return (mae_train_avg, mae_test_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_features(\n",
    "        model: XGBRegressor, \n",
    "        X: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove features less important than a reference 'random_feature'.\n",
    "    \n",
    "    Args:\n",
    "        model (XGBRegressor): XGBoost regressor using scikit-learn API.\n",
    "        X (pd.DataFrame): Features DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with redundant features removed.\n",
    "    \"\"\"\n",
    "    # Get feature importances and feature names\n",
    "    importances = model.feature_importances_\n",
    "    features = X.columns.tolist()\n",
    "    \n",
    "    # Create a DataFrame for feature importances\n",
    "    feat_imps = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort features by importance\n",
    "    sorted_feat_imps = feat_imps.sort_values(by='importance', ascending=False)\n",
    "    sorted_feat_imps.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Find the importance threshold of 'random_feature'\n",
    "    random_feature_row = sorted_feat_imps[\n",
    "            sorted_feat_imps['feature'] == 'random_feature']\n",
    "    thresh = random_feature_row['importance'].iloc[0]\n",
    "    \n",
    "    # Identify columns to drop (less important than 'random_feature')\n",
    "    redundant_features = sorted_feat_imps[\n",
    "            sorted_feat_imps['importance'] < thresh]\n",
    "    to_drop = redundant_features['feature'].tolist()\n",
    "    to_drop.append('random_feature')\n",
    "    \n",
    "    # Drop redundant columns\n",
    "    X_new = X.drop(columns=to_drop)\n",
    "    \n",
    "    return (X_new, to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X, y, n_trials=100, n_splits=5):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 2, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 15),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'alpha': trial.suggest_float(\"alpha\", 1e-5, 10, log=True),  \n",
    "            'gamma': trial.suggest_float(\"gamma\", 1e-5, 5, log=True),\n",
    "            'lambda': trial.suggest_float('lambda', 1e-5, 10.0, log=True),\n",
    "            'early_stopping_rounds': 50\n",
    "        }\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "        mae_train_avg, mae_test_avg = train(model, X, y, n_splits)\n",
    "        return mae_test_avg\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "\n",
    "    # Optimize the study, the objective function is passed in as the first argument\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Results\n",
    "    print('Number of finished trials: ', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    \n",
    "    print('Value: ', trial.value)\n",
    "    print('Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'    {key}: {value}')\n",
    "    \n",
    "    return trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_last_row_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(n_trials=100) -> XGBRegressor:\n",
    "    # Load the data.\n",
    "    filename = 'data/2018-01-01-2022-01-01.bin'\n",
    "    df = read_data(filename)\n",
    "\n",
    "    # Clean it, extract date features and running statistics.\n",
    "    df_no_outliers = filter_by_iqr(df)\n",
    "    df_newfeat= extract_features(df_no_outliers)\n",
    "    \n",
    "    # Preprocess the dataset for model input.\n",
    "    df_transformed = transform_to_supervised(df_newfeat)\n",
    "    df_encoded, ohe = encode_categorical(df_transformed)\n",
    "    \n",
    "    # Separate features and target.\n",
    "    X = df_encoded.drop(columns=['value'])\n",
    "    y = df_encoded.value\n",
    "\n",
    "    most_recent_vals = df_encoded.iloc[-1][['rolling_mean', 'rolling_std']]\n",
    "\n",
    "    model = XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "    n_splits = 9\n",
    "    mae_train_avg, mae_test_avg = train(model, X, y, n_splits=n_splits)\n",
    "    print(f'Avg MAE:\\nTrain: {mae_train_avg}\\nTest: {mae_test_avg}')\n",
    "    \n",
    "    X_new, features_to_drop = remove_redundant_features(model, X)\n",
    "    schema = X_new.columns.tolist()\n",
    "    best_params = tune_hyperparameters(X_new, y, n_trials=n_trials)\n",
    "    model = XGBRegressor(**best_params)\n",
    "    train(model, X_new, y, n_splits=n_splits)\n",
    "\n",
    "    artifacts = [ohe, features_to_drop, schema, most_recent_vals]\n",
    "    return (model, artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, artifacts = main()\n",
    "ohe, features_to_drop, schema, most_recent_vals = artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/2018-01-01-2022-01-01.bin'\n",
    "data_inference = read_data(filename)\n",
    "df_inference = data_inference.tail(1)\n",
    "df_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_inference(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = extract_features(df, inference=True)\n",
    "    df = df.rename(columns={'value': 'lag'})\n",
    "    df, _ = encode_categorical(df, ohe=ohe, fit=False)\n",
    "    df[['rolling_mean', 'rolling_std']] = most_recent_vals\n",
    "    X_recent = df[schema]\n",
    "    return X_recent\n",
    "\n",
    "X_recent = prepare_for_inference(df_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_recent)\n",
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
