{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import optuna\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads data from a pickle file, removes unused columns.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Pandas DataFrame object.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO Verify data format and pre-defined schema.\n",
    "\n",
    "    cols_to_drop = ['subba-name', 'parent', 'parent-name', 'value-units']\n",
    "    dtypes = {'period': 'datetime64',\n",
    "              'subba': 'category',\n",
    "              'timezone': 'category',\n",
    "              'value': 'uint64'}\n",
    "\n",
    "    # Load the data, remove extra columns, and adjust data types.\n",
    "    df = pd.read_pickle(filename).drop(columns=cols_to_drop)\n",
    "    df = df.drop_duplicates().dropna().reset_index(drop=True)\n",
    "    df = df.astype(dtypes)\n",
    "    \n",
    "    # We focus solely on New York City data for our task.\n",
    "    df = df[df['subba'] == 'ZONJ']\n",
    "    return df\n",
    "\n",
    "\n",
    "# print('df shape:', df.shape)\n",
    "# print(df.head(1))\n",
    "# print(df.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_iqr(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter DataFrame using the Interquartile Range (IQR) method.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the \"value\" column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    # Compute the IQR for the \"value\" column\n",
    "    Q1 = df['value'].quantile(0.25)\n",
    "    Q3 = df['value'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define bounds for the acceptable range\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Remove rows where the \"value\" column is outside the bounds.\n",
    "    filt = (df['value'] >= lower_bound) & (df['value'] <= upper_bound)\n",
    "    df_filtered = df[filt]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def plot_data(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Plot the data from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'period' and 'value' columns.\n",
    "    \"\"\"\n",
    "    # Sort the DataFrame by \"period\" and plot the data\n",
    "    x = 'period'\n",
    "    y = 'value'\n",
    "    \n",
    "    df.sort_values(by='period').plot(x=x, y=y, figsize=(10, 5))\n",
    "\n",
    "    # Set the axis labels and title\n",
    "    plt.xlabel('Period')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Value over Time')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# plot_data(df_no_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "        df: pd.DataFrame,\n",
    "        window_size=7\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract date-related and rolling statistics features from DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'period' and 'value' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added date features and rolling stats.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Separate date features\n",
    "    df['year'] = df['period'].dt.year.astype('uint16')\n",
    "    df['month'] = df['period'].dt.month.astype('uint16')\n",
    "    df['day'] = df['period'].dt.day.astype('uint16')\n",
    "\n",
    "    # Extract the day of the week\n",
    "    df['day_of_week'] = df['period'].dt.day_name().astype('category')\n",
    "\n",
    "    # Extract quarterly and weekly information to capture seasonality\n",
    "    df['quarter'] = df['period'].dt.quarter.astype('uint8')\n",
    "    df['week_of_year'] = df['period'].dt.isocalendar().week.astype('uint16')\n",
    "\n",
    "    # Mark the weekends\n",
    "    df['is_weekend'] = (df['period'].dt.weekday >= 5).astype('uint8')\n",
    "    \n",
    "    # Create rolling mean feature\n",
    "    df['rolling_mean'] = df['value'].rolling(window=window_size).mean()\n",
    "    \n",
    "    # Create rolling standard deviation feature\n",
    "    df['rolling_std'] = df['value'].rolling(window=window_size).std()\n",
    "    \n",
    "    # Remove NaNs introduced by the rolling mean and std\n",
    "    df = df.dropna().copy().reset_index(drop=True)\n",
    "    print('df after dropna',df)\n",
    "\n",
    "    # Create a random feature as a threshold for later feature filtering\n",
    "    random_feature = [random.random() for _ in range(df.shape[0])]\n",
    "    df['random_feature'] = random_feature\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_supervised(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform DataFrame into a supervised learning format.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'lag' column and 'period' dropped.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create 'lag' column by shifting 'value' column\n",
    "    df.loc[:, 'lag'] = df['value'].shift()\n",
    "\n",
    "    # Remove rows with missing values (NaN) due to the shift\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Convert 'lag' column to unsigned 64-bit integer type\n",
    "    df.lag = df.lag.astype('uint64')\n",
    "\n",
    "    # Drop the 'period' column from the DataFrame\n",
    "    df = df.drop(columns=['period'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_categorical(df: pd.DataFrame, ohe=None, fit=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encode categorical columns as dummy variables.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with categorical columns encoded as dummy variables.\n",
    "    \"\"\" \n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    if ohe == None:\n",
    "        ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    # List of columns to encode as dummy variables\n",
    "    feat_to_enc = df.select_dtypes('category')\n",
    "\n",
    "    # Get dummy variables for the specified columns\n",
    "    if fit == True:\n",
    "        dummies = pd.DataFrame(ohe.fit_transform(feat_to_enc))\n",
    "    else:\n",
    "        dummies = pd.DataFrame(ohe.transform(feat_to_enc))\n",
    "    dummies.columns = ohe.get_feature_names_out()\n",
    "    \n",
    "    # Concatenate the dummy variables with the original DataFrame\n",
    "    df = pd.concat((df.drop(columns=feat_to_enc.columns), dummies), axis=1)\n",
    "    return df, ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: XGBRegressor, \n",
    "    X: pd.DataFrame, \n",
    "    y: pd.Series,\n",
    "    n_splits: int\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train a model using TimeSeriesSplit cross-validation.\n",
    "\n",
    "    Args:\n",
    "        model (XGBRegressor): XGBoost regressor using scikit-learn API.\n",
    "        X (pd.DataFrame): Features DataFrame.\n",
    "        y (pd.Series): Target values Series.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains average Mean Absolute Error on the training \n",
    "              set (float) and test set (float) respectively.\n",
    "    \"\"\"\n",
    "    # Lists to store mean absolute errors for training and test sets\n",
    "    mae_train_hist = []\n",
    "    mae_test_hist = []\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    # Split the data using TimeSeriesSplit for cross-validation\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        # Split the data into training and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "        \n",
    "        # Predict on training and test sets\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Calculate mean absolute error for training and test sets\n",
    "        mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "        mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "        \n",
    "        # Append the errors to their respective histories\n",
    "        mae_train_hist.append(mae_train)\n",
    "        mae_test_hist.append(mae_test)\n",
    "\n",
    "    # Calculate average mean absolute error for training and test sets\n",
    "    mae_train_avg = sum(mae_train_hist) / len(mae_train_hist)\n",
    "    mae_test_avg = sum(mae_test_hist) / len(mae_test_hist)\n",
    "    \n",
    "    return (mae_train_avg, mae_test_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_features(\n",
    "        model: XGBRegressor, \n",
    "        X: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove features less important than a reference 'random_feature'.\n",
    "    \n",
    "    Args:\n",
    "        model (XGBRegressor): XGBoost regressor using scikit-learn API.\n",
    "        X (pd.DataFrame): Features DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with redundant features removed.\n",
    "    \"\"\"\n",
    "    # Get feature importances and feature names\n",
    "    importances = model.feature_importances_\n",
    "    features = X.columns.tolist()\n",
    "    \n",
    "    # Create a DataFrame for feature importances\n",
    "    feat_imps = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort features by importance\n",
    "    sorted_feat_imps = feat_imps.sort_values(by='importance', ascending=False)\n",
    "    sorted_feat_imps.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Find the importance threshold of 'random_feature'\n",
    "    random_feature_row = sorted_feat_imps[\n",
    "            sorted_feat_imps['feature'] == 'random_feature']\n",
    "    thresh = random_feature_row['importance'].iloc[0]\n",
    "    \n",
    "    # Identify columns to drop (less important than 'random_feature')\n",
    "    redundant_features = sorted_feat_imps[\n",
    "            sorted_feat_imps['importance'] < thresh]\n",
    "    to_drop = redundant_features['feature'].tolist()\n",
    "    to_drop.append('random_feature')\n",
    "    \n",
    "    # Drop redundant columns\n",
    "    X_new = X.drop(columns=to_drop)\n",
    "    \n",
    "    return (X_new, to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X, y, n_trials=100, n_splits=5):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 2, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 15),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'alpha': trial.suggest_float(\"alpha\", 1e-5, 10, log=True),  \n",
    "            'gamma': trial.suggest_float(\"gamma\", 1e-5, 5, log=True),\n",
    "            'lambda': trial.suggest_float('lambda', 1e-5, 10.0, log=True),\n",
    "            'early_stopping_rounds': 50\n",
    "        }\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "        mae_train_avg, mae_test_avg = train(model, X, y, n_splits)\n",
    "        return mae_test_avg\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "\n",
    "    # Optimize the study, the objective function is passed in as the first argument\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Results\n",
    "    print('Number of finished trials: ', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    \n",
    "    print('Value: ', trial.value)\n",
    "    print('Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'    {key}: {value}')\n",
    "    \n",
    "    return trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(n_trials=1) -> XGBRegressor:\n",
    "    # Load the data.\n",
    "    filename = 'data/2018-01-01-2022-01-01.bin'\n",
    "    df = read_data(filename)\n",
    "\n",
    "    # Clean it, extract date features and running statistics.\n",
    "    df_no_outliers = filter_by_iqr(df)\n",
    "    df_newfeat= extract_features(df_no_outliers)\n",
    "    \n",
    "    # Preprocess the dataset for model input.\n",
    "    df_transformed = transform_to_supervised(df_newfeat)\n",
    "    df_encoded, ohe = encode_categorical(df_transformed)\n",
    "    \n",
    "    # Separate features and target.\n",
    "    X = df_encoded.drop(columns=['value'])\n",
    "    y = df_encoded.value\n",
    "\n",
    "    model = XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "    n_splits = 9\n",
    "    mae_train_avg, mae_test_avg = train(model, X, y, n_splits=n_splits)\n",
    "    print(f'Avg MAE:\\nTrain: {mae_train_avg}\\nTest: {mae_test_avg}')\n",
    "    \n",
    "    X_new, features_to_drop = remove_redundant_features(model, X)\n",
    "    \n",
    "    best_params = tune_hyperparameters(X_new, y, n_trials=n_trials)\n",
    "    model = XGBRegressor(**best_params)\n",
    "    train(model, X_new, y, n_splits=n_splits)\n",
    "\n",
    "    return model, ohe, features_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df after dropna          period subba  timezone   value  year  month  day day_of_week  \\\n",
      "0    2021-12-31  ZONJ   Arizona  117623  2021     12   31      Friday   \n",
      "1    2021-12-31  ZONJ  Mountain  117623  2021     12   31      Friday   \n",
      "2    2021-12-31  ZONJ   Central  117593  2021     12   31      Friday   \n",
      "3    2021-12-31  ZONJ   Eastern  117546  2021     12   31      Friday   \n",
      "4    2021-12-30  ZONJ   Eastern  123116  2021     12   30    Thursday   \n",
      "...         ...   ...       ...     ...   ...    ...  ...         ...   \n",
      "6034 2018-06-26  ZONJ   Pacific  154798  2018      6   26     Tuesday   \n",
      "6035 2018-06-26  ZONJ  Mountain  154792  2018      6   26     Tuesday   \n",
      "6036 2018-06-26  ZONJ   Eastern  154928  2018      6   26     Tuesday   \n",
      "6037 2018-06-26  ZONJ   Central  154836  2018      6   26     Tuesday   \n",
      "6038 2018-06-26  ZONJ   Arizona  154798  2018      6   26     Tuesday   \n",
      "\n",
      "      quarter  week_of_year  is_weekend   rolling_mean  rolling_std  \n",
      "0           4            52           0  113067.714286  3116.987419  \n",
      "1           4            52           0  114011.714286  3381.141411  \n",
      "2           4            52           0  114928.428571  3353.964811  \n",
      "3           4            52           0  115813.000000  3057.568587  \n",
      "4           4            52           0  117518.714286  3355.546787  \n",
      "...       ...           ...         ...            ...          ...  \n",
      "6034        2            26           0  155650.000000  7799.073428  \n",
      "6035        2            26           0  153019.000000  1219.004649  \n",
      "6036        2            26           0  153372.428571  1376.378564  \n",
      "6037        2            26           0  153734.571429  1380.975363  \n",
      "6038        2            26           0  154114.571429  1225.669729  \n",
      "\n",
      "[6039 rows x 13 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-07 13:22:13,793] A new study created in memory with name: no-name-0a8420b4-2245-448e-84a5-095edad7a7b5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg MAE:\n",
      "Train: 398.1347730885192\n",
      "Test: 3231.9671017021374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-07 13:22:14,421] Trial 0 finished with value: 3506.428125 and parameters: {'n_estimators': 44, 'max_depth': 15, 'learning_rate': 0.14756516106505638, 'subsample': 0.7078301349974585, 'colsample_bytree': 0.5321121017741631, 'min_child_weight': 6, 'alpha': 4.3860756369466297e-05, 'gamma': 0.009759538884952212, 'lambda': 6.397801747345744e-05}. Best is trial 0 with value: 3506.428125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  1\n",
      "Best trial:\n",
      "Value:  3506.428125\n",
      "Params: \n",
      "    n_estimators: 44\n",
      "    max_depth: 15\n",
      "    learning_rate: 0.14756516106505638\n",
      "    subsample: 0.7078301349974585\n",
      "    colsample_bytree: 0.5321121017741631\n",
      "    min_child_weight: 6\n",
      "    alpha: 4.3860756369466297e-05\n",
      "    gamma: 0.009759538884952212\n",
      "    lambda: 6.397801747345744e-05\n"
     ]
    }
   ],
   "source": [
    "model, ohe, features_to_drop = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_inference(df: pd.DataFrame):\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After loading: (1, 4)\n",
      "df after dropna       period subba timezone   value  year  month  day day_of_week  quarter  \\\n",
      "0 2022-01-02  ZONJ  Pacific  112878  2022      1    2      Sunday        1   \n",
      "\n",
      "   week_of_year  is_weekend   rolling_mean  rolling_std  \n",
      "0            52           1  154114.571429  1225.669729  \n",
      "      period subba timezone   value  year  month  day day_of_week  quarter  \\\n",
      "0 2022-01-02  ZONJ  Pacific  112878  2022      1    2      Sunday        1   \n",
      "\n",
      "   week_of_year  is_weekend   rolling_mean  rolling_std  random_feature  \n",
      "0            52           1  154114.571429  1225.669729        0.888663  \n",
      "After feature extraction: (1, 14)\n",
      "After transform: (0, 14)\n",
      "['subba', 'timezone', 'value', 'year', 'month', 'day', 'day_of_week', 'quarter', 'week_of_year', 'is_weekend', 'rolling_mean', 'rolling_std', 'random_feature', 'lag']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- subba\n- timezone\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAfter transform:\u001b[39m\u001b[39m'\u001b[39m, df_transformed\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(df_transformed\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mtolist())\n\u001b[0;32m---> 18\u001b[0m df_encoded, _ \u001b[39m=\u001b[39m encode_categorical(df_transformed, ohe\u001b[39m=\u001b[39;49mohe, fit\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAfter encoding:\u001b[39m\u001b[39m'\u001b[39m, df_encoded\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     21\u001b[0m to_drop \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m features_to_drop\n",
      "Cell \u001b[0;32mIn[83], line 48\u001b[0m, in \u001b[0;36mencode_categorical\u001b[0;34m(df, ohe, fit)\u001b[0m\n\u001b[1;32m     46\u001b[0m     dummies \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(ohe\u001b[39m.\u001b[39mfit_transform(feat_to_enc))\n\u001b[1;32m     47\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     dummies \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(ohe\u001b[39m.\u001b[39;49mtransform(feat_to_enc))\n\u001b[1;32m     49\u001b[0m dummies\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m ohe\u001b[39m.\u001b[39mget_feature_names_out()\n\u001b[1;32m     51\u001b[0m \u001b[39m# Concatenate the dummy variables with the original DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/forecast-env/lib/python3.9/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/forecast-env/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:1016\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[39m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m warn_on_unknown \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_unknown \u001b[39min\u001b[39;00m {\n\u001b[1;32m   1013\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1014\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minfrequent_if_exist\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1015\u001b[0m }\n\u001b[0;32m-> 1016\u001b[0m X_int, X_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(\n\u001b[1;32m   1017\u001b[0m     X,\n\u001b[1;32m   1018\u001b[0m     handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown,\n\u001b[1;32m   1019\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1020\u001b[0m     warn_on_unknown\u001b[39m=\u001b[39;49mwarn_on_unknown,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1023\u001b[0m n_samples, n_features \u001b[39m=\u001b[39m X_int\u001b[39m.\u001b[39mshape\n\u001b[1;32m   1025\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_drop_idx_after_grouping \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/forecast-env/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:179\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[0;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform\u001b[39m(\n\u001b[1;32m    172\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    173\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     ignore_category_indices\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m ):\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_feature_names(X, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    180\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    181\u001b[0m     X_list, n_samples, n_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X(\n\u001b[1;32m    182\u001b[0m         X, force_all_finite\u001b[39m=\u001b[39mforce_all_finite\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/forecast-env/lib/python3.9/site-packages/sklearn/base.py:506\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m missing_names \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    502\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    503\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\n\u001b[0;32m--> 506\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- subba\n- timezone\n"
     ]
    }
   ],
   "source": [
    "# Test inference on unseen data\n",
    "fname = 'testdata/2022-01-02-2022-01-02.bin'\n",
    "# fname = 'data/2018-01-01-2022-01-01.bin'\n",
    "test_df = read_data(fname).iloc[0,:].to_frame().T.reset_index(drop=True)\n",
    "# print(test_df)\n",
    "print('After loading:', test_df.shape)\n",
    "df_newfeat, _ = extract_features(\n",
    "    test_df, \n",
    "    window_size=1, \n",
    "    inference=True, \n",
    "    rolling_stats=saved_rolling_stats\n",
    ")\n",
    "print(df_newfeat)\n",
    "print('After feature extraction:', df_newfeat.shape)\n",
    "df_transformed = transform_to_supervised(df_newfeat)\n",
    "print('After transform:', df_transformed.shape)\n",
    "print(df_transformed.columns.tolist())\n",
    "df_encoded, _ = encode_categorical(df_transformed, ohe=ohe, fit=False)\n",
    "print('After encoding:', df_encoded.shape)\n",
    "\n",
    "to_drop = ['value'] + features_to_drop\n",
    "X = df_encoded.drop(columns=to_drop)\n",
    "print('After separating features from target:',X.shape)\n",
    "y_true = df_encoded.value\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "mean_absolute_error(y_true, y_pred)\n",
    "print('y_true:', y_true, '\\ny_pred:', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
